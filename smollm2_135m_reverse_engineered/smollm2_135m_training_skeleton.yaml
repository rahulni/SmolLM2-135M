model:
  architecture: transformer_decoder
  base_model_type: llama
  hidden_size: 576
  intermediate_size: 1536
  num_layers: 30
  num_attention_heads: 9
  num_key_value_heads: 3
  vocab_size: 49152
  max_position_embeddings: 8192
  rms_norm_eps: 1.0e-05
  rope_theta: 100000.0
  rope_scaling: null
  tie_word_embeddings: true
tokens:
  bos_token_id: 0
  eos_token_id: 0
  pad_token_id: null
training:
  total_tokens: 2000000000000
  global_batch_size_tokens: 1000000
  sequence_length: 8192
  precision: bf16
  optimizer:
    type: adamw
    betas:
    - 0.9
    - 0.95
    eps: 1.0e-08
    weight_decay: 0.1
  lr_schedule:
    type: cosine
    max_lr: 0.0003
    min_lr: 3.0e-05
    warmup_ratio: 0.01
data:
  train_datasets:
  - name: fineweb-edu
    sampling_ratio: 0.25
  - name: dclm
    sampling_ratio: 0.25
  - name: the-stack
    sampling_ratio: 0.25
  - name: other-filtered-data
    sampling_ratio: 0.25
  tokenizer: HuggingFaceTB/SmolLM2-135M
